{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akankshakusf/Project-Fine-Tuned-LLMs-for-Product-Pricing/blob/master/Evaluate_My_Model_Predict_Product_Prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Predict Product Prices\n",
        "\n",
        "### And now, to evaluate my  fine-tuned open source model"
      ],
      "metadata": {
        "id": "NjmFVTeYsTtB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cPbDPUAsIdZ"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "!pip install -q datasets \\\n",
        "                requests \\\n",
        "                torch \\\n",
        "                peft \\\n",
        "                bitsandbytes \\\n",
        "                transformers \\\n",
        "                trl \\\n",
        "                accelerate \\\n",
        "                sentencepiece\\\n",
        "                tiktoken \\\n",
        "                matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import packages\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,set_seed\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from datetime import datetime\n",
        "from peft import PeftModel\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "txj6boXl6B4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import increment_lineno\n",
        "#contants\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "PROJECT_NAME = \"pricer\"\n",
        "HF_USER = \"akankshakusf\"\n",
        "\n",
        "#fine tuned model\n",
        "RUN_NAME = \"2025-05-02_20.00.01\"\n",
        "PROJECT_RUN_NAME =  f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
        "REVISION = \"5f1cd803dd15a87b779f73bc4061081e50e9e064\" # or REVISION = None\n",
        "FINETUNED_MODEL = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "#my data from hugging face\n",
        "DATASET_NAME = f\"{HF_USER}/pricer-data\"\n",
        "\n",
        "\n",
        "#hyperparameters for QLoRA\n",
        "QUANT_4_BIT = True\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "#used for writing to output in color\n",
        "\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "RED = \"\\033[91m\"\n",
        "RESET = \"\\033[0m\"\n",
        "COLOR_MAP = {\"red\":RED, \"orange\": YELLOW, \"green\": GREEN}"
      ],
      "metadata": {
        "id": "YfVo8OUE6B1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#log in to HuggingFace\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "MBp2n6XA6Byv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(DATASET_NAME)\n",
        "train = dataset['train']\n",
        "test = dataset['test']"
      ],
      "metadata": {
        "id": "eAlX4eg-6BwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "train= dataset['train']\n",
        "test = dataset['test']#define quantization needed\n",
        "if QUANT_4_BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16\n",
        "  )"
      ],
      "metadata": {
        "id": "7MnHi2q36Bs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Tokenizer and the Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Load the fine-tuned model with PEFT\n",
        "if REVISION:\n",
        "  fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL, revision=REVISION)\n",
        "else:\n",
        "  fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)\n",
        "\n",
        "\n",
        "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ],
      "metadata": {
        "id": "vt2DzpPB6BqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J0Pgt1Ju6BnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z8mkNnZo6Bgc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}