{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2FywAxojv1ir0zP17QVop",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akankshakusf/Project-Fine-Tuned-LLMs-for-Product-Pricing/blob/master/Performing_LoRA_and_QLoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing LoRA and QLoRA Techniques"
      ],
      "metadata": {
        "id": "fdUgRKjBjsAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict Product Prices"
      ],
      "metadata": {
        "id": "JCSN5WJFjplD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g_-lfKPRjUEe"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "!pip install -q datasets \\\n",
        "                requests \\\n",
        "                torch \\\n",
        "                peft \\\n",
        "                bitsandbytes \\\n",
        "                transformers \\\n",
        "                trl \\\n",
        "                accelerate \\\n",
        "                sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference :https://huggingface.co/docs/peft/en/quicktour\n",
        "- https://huggingface.co/docs/peft/en/task_guides/lora_based_methods"
      ],
      "metadata": {
        "id": "C7vV8qbeguWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import packages\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\\\n",
        "                          TrainingArguments, set_seed\n",
        "from peft import LoraConfig, PeftModel\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "Kgx9udSZj-13"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model constants\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "FINETUNED_MODEL = f\"ed-donner/pricer-2024-09-13_13.04.39\" # this is yet to be made"
      ],
      "metadata": {
        "id": "hJH4ZdDlj-zl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters for Qlora finetuning\n",
        "LORA_R=32\n",
        "LORA_ALPHA=64  #RFT: double of R\n",
        "TARGET_MODULES= [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"] #attention heads layer"
      ],
      "metadata": {
        "id": "vgQGkmfqj-w9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#log in to HuggingFace\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "hU-yrACsj-sR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying out different Quantization"
      ],
      "metadata": {
        "id": "NTNYyHbbt8hA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Model"
      ],
      "metadata": {
        "id": "G-MIqHVluXaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the base model from hugging face\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL,device_map=\"auto\" )"
      ],
      "metadata": {
        "id": "i4qmBJL-j-kN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Restart your session here and clear gpu cache!"
      ],
      "metadata": {
        "id": "KoQl86pboO0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#see memory foot print\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ],
      "metadata": {
        "id": "nL7IJW76l7fS",
        "outputId": "13ae881a-0cb4-487a-9455-6ab5a6068d79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory footprint: 32.1 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#review the model\n",
        "base_model"
      ],
      "metadata": {
        "id": "nP4RX9R7mMA3",
        "outputId": "0d8bc5f7-fe49-4a41-d6ef-57bb3049d9eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Restart your session here and clear gpu cache!"
      ],
      "metadata": {
        "id": "gsvQLpNprFYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Model using 8bit"
      ],
      "metadata": {
        "id": "PQef7g5TuU-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Base Model using 8 bit\n",
        "\n",
        "#define the level if quantization needed\n",
        "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config, ###quant config u defined\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "3bXEFuzOuDsw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check size of model\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ],
      "metadata": {
        "id": "TgFHrG8suDo7",
        "outputId": "0a2d148b-d94d-4ef7-f446-c3105f6dfbbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory footprint: 9.1 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#review model\n",
        "base_model"
      ],
      "metadata": {
        "id": "BTiLFsvLuDl_",
        "outputId": "24449ead-6090-4f32-d85e-a4c620469c26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Model using 4bit"
      ],
      "metadata": {
        "id": "SPiDcTv-uM3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the tokenizer and the base model using 4bit\n",
        "\n",
        "#define the level if quantization needed\n",
        "quant_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                  bnb_4bit_use_double_quant=True,\n",
        "                                  bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                                  bnb_4bit_quant_type=\"nf4\") ##normal distribution\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config, ###quant config u defined\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "stXrxMHAqk8S"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check size of model\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint()/1e9:,.2f} GB\")"
      ],
      "metadata": {
        "id": "RTNNjpjgqk5Z",
        "outputId": "0c485f1e-e76b-42f8-d9fb-53b572ae8d51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory footprint: 5.59 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#review model\n",
        "base_model"
      ],
      "metadata": {
        "id": "F537-m8bqk27",
        "outputId": "778003fc-ca93-41bd-9a99-97826b44a40a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA Fine-Tuned LLaMA Model"
      ],
      "metadata": {
        "id": "V7_KdKg-wAsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model= PeftModel.from_pretrained(base_model,FINETUNED_MODEL )"
      ],
      "metadata": {
        "id": "A42V_d4Dqkz2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check size of model\n",
        "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ],
      "metadata": {
        "id": "NklqHcMmqkw1",
        "outputId": "eb77821b-7d42-4f2c-eea0-fcb9416b1644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory footprint: 9.19 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#review the model\n",
        "fine_tuned_model"
      ],
      "metadata": {
        "id": "pFzNACLZqkti",
        "outputId": "6bbd4d13-e628-46ec-d4d8-9f7e9fd43a31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear8bitLt(\n",
              "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear8bitLt(\n",
              "                (base_layer): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear8bitLt(\n",
              "                (base_layer): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear8bitLt(\n",
              "                (base_layer): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Dimension Calculus for LoRA Fine-Tuned LLaMA Model"
      ],
      "metadata": {
        "id": "fg9JpSd0xzov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Each of the Target Modules has 2 LoRA Adaptor matrices, called lora_A and lora_B\n",
        "# These are designed so that weights can be adapted by adding alpha * lora_A * lora_B\n",
        "# Let's count the number of weights using their dimensions:\n",
        "\n",
        "#see the matrix dimensions above\n",
        "lora_q_proj = 4096 * 32 + 4096 * 32\n",
        "lora_k_proj = 4096 * 32 + 1024 * 32\n",
        "lora_v_proj = 4096 * 32 + 1024 * 32\n",
        "lora_o_proj = 4096 * 32 + 4096 * 32\n",
        "\n",
        "#each layer comes to\n",
        "lora_layer = lora_q_proj+lora_k_proj+lora_v_proj+lora_o_proj\n",
        "\n",
        "#there are 32 layers\n",
        "params = lora_layer * 32\n",
        "\n",
        "#so the total size in MB is\n",
        "# multiply 4 as 4 proj layers\n",
        "size = (params*4) /1_000_000\n",
        "\n",
        "print(f\"Total number of params: {params:,} and size {size:,.1f}MB\")"
      ],
      "metadata": {
        "id": "a2MO99c5qkqM",
        "outputId": "1d89c806-76cb-4a73-f9f4-01d9fab797ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of params: 27,262,976 and size 109.1MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZCz6XyNzWws"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}